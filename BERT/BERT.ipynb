{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "path = \"/global/cscratch1/sd/ajaybati/pickles/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/common/software/python/3.7-anaconda-2019.10/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "noteevents = pd.read_csv(\"/project/projectdirs/m1532/Projects_MVP/_datasets/mimiciii/NOTEEVENTS.csv\")\n",
    "\n",
    "\n",
    "final_df = pd.read_pickle(path+\"final_df.pickle\")\n",
    "notes_df = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = final_df[[\"HADM_ID\",\"SUBJECT_ID\",\"DOB\"]].drop_duplicates().merge(notes_df, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"right\")\n",
    "\n",
    "notes = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = pd.merge(final_df[[\"SUBJECT_ID\",\"HADM_ID\",\"DOB\"]],notes.drop_duplicates(subset = [\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\"]), on=[\"SUBJECT_ID\",\"HADM_ID\"], how = \"left\")\n",
    "notes_df = notes_df.drop_duplicates(subset='TEXT')\n",
    "notes_df\n",
    "\n",
    "salience_patient = pd.read_pickle(path+\"salience_patient.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data collection\n",
    "import random\n",
    "class textLoader(Dataset):\n",
    "    def __init__(self,transform = None, yes = True):\n",
    "        self.input_ids_all = []\n",
    "        self.attention_masks_all = []\n",
    "        self.text = notes_df[\"TEXT\"].tolist()\n",
    "        self.samples = len(self.text)\n",
    "        total = 0\n",
    "        x=0\n",
    "        for note in self.text[250000:]:\n",
    "            start = time.time()\n",
    "            try:\n",
    "                for sent in sent_tokenize(note):\n",
    "                    \n",
    "                    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,\n",
    "                        truncation = True,# Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "                    # Add the encoded sentence to the list.    \n",
    "                    input_ids = encoded_dict['input_ids']\n",
    "                    attention_masks = encoded_dict['attention_mask']\n",
    "\n",
    "                    if yes:\n",
    "                        input_ids_real = []\n",
    "                        for index,word in enumerate(input_ids[0]):\n",
    "                            if int(word) != 101 and int(word) != 102:\n",
    "                                rando = random.random()\n",
    "                                random.seed()\n",
    "                                if rando < 0.2 and int(word)!=0:\n",
    "                                    input_ids_real.append(103)\n",
    "                                else:\n",
    "                                    input_ids_real.append(int(word))\n",
    "                            else:\n",
    "                                input_ids_real.append(int(word))\n",
    "                        input_ids_real = torch.tensor(input_ids_real).view(1,64)\n",
    "\n",
    "                        self.input_ids_all.append(input_ids_real)\n",
    "                        self.attention_masks_all.append(attention_masks)\n",
    "                    else:\n",
    "\n",
    "                        self.input_ids_all.append(input_ids)\n",
    "                        self.attention_masks_all.append(attention_masks)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(str(e), input_ids)\n",
    "            end = time.time()\n",
    "            if((self.text.index(note)+1)%10000==0):\n",
    "                x+=1\n",
    "                total+=(end-start)\n",
    "                print((self.text.index(note)+1)/367101+0.6810114927499517, total/(x))\n",
    "       \n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.input_ids_all[index],self.attention_masks_all[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    \n",
    "print(\"=\"*20,\"data randomized 0.6810114927499517+\",\"=\"*20)\n",
    "\n",
    "data_2 = textLoader(yes = True)\n",
    "print(\"done\")\n",
    "data_input_randomized = torch.cat(tuple(data_2.input_ids_all),dim = 0)\n",
    "attention_masks_randomized = torch.cat(tuple(data_2.attention_masks_all),dim = 0)\n",
    "print(\"save 1\")\n",
    "torch.save(data_input_randomized,\"pickles/input_ids_randomized30.pickle\")\n",
    "print(\"save 2\")\n",
    "torch.save(attention_masks_randomized,\"pickles/attention_masks_randomized30.pickle\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8119516\n"
     ]
    }
   ],
   "source": [
    "input_ids_randomized70 = torch.load(path+\"input_ids_randomized70.pickle\")\n",
    "attention_masks_randomized70 = torch.load(path+\"attention_masks_randomized70.pickle\")\n",
    "REDOinput_ids_randomized30 = torch.load(path+\"REDOinput_ids_randomized30.pickle\")\n",
    "REDOattention_masks_randomized30 = torch.load(path+\"REDOattention_masks_randomized30.pickle\")\n",
    "print(len(input_ids_randomized70)+len(REDOinput_ids_randomized30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8119516"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing data before save\n",
    "fullist2 = list(attention_masks_randomized70)\n",
    "for x in REDOattention_masks_randomized30:\n",
    "    fullist2.append(x)\n",
    "\n",
    "\n",
    "fullist = list(input_ids_randomized70)\n",
    "for x in REDOinput_ids_randomized30:\n",
    "    fullist.append(x)\n",
    "\n",
    "\n",
    "fullist_changed2 = []\n",
    "for tens in fullist2:\n",
    "    tens = tens.view(1,64)\n",
    "    fullist_changed2.append(tens)\n",
    "fullist_changed2\n",
    "\n",
    "fullist_changed = []\n",
    "for tens in fullist:\n",
    "    tens = tens.view(1,64)\n",
    "    fullist_changed.append(tens)\n",
    "fullist_changed\n",
    "\n",
    "attention_masks_randomized100 = torch.cat(tuple(fullist_changed2),0)\n",
    "len(attention_masks_randomized100)\n",
    "\n",
    "input_ids_randomized100 = torch.cat(tuple(fullist_changed),0)\n",
    "len(input_ids_randomized100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_all = torch.load(path+\"input_ids_randomized100.pickle\")\n",
    "attention_masks_all = torch.load(path+\"attention_masks_randomized100.pickle\")\n",
    "input_ids_real = torch.load(path+\"input_ids_real.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 21442, 26915,  6633,  2318,   103,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "tensor([  101, 21442, 26915,  6633,  2318,  2651,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_all[5501010])\n",
    "print(input_ids_real[5501010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_indices = []\n",
    "# x=0\n",
    "# for sent in input_ids_all:\n",
    "#     start=time.time()\n",
    "#     sentence = list(sent)\n",
    "#     mask = []\n",
    "#     for word in sentence:\n",
    "#         if word==103:\n",
    "#             mask.append(index)\n",
    "#     mask_indices.append(mask)\n",
    "#     end = time.time()\n",
    "#     x+=1\n",
    "#     if x%500000==0:\n",
    "#         print(\"=\"*20+\" \"+str(x)+\"/8286324\"+\"=\"*20, len(mask)/len(masked_indices))\n",
    "with open(path+\"mask_indices.pickle\",\"rb\") as f:\n",
    "    mask_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================1000000/8119516====================\n",
      "0.00010085105895996094\n",
      "====================2000000/8119516====================\n",
      "0.00011396408081054688\n",
      "====================3000000/8119516====================\n",
      "0.00010228157043457031\n",
      "====================4000000/8119516====================\n",
      "0.0001289844512939453\n",
      "====================5000000/8119516====================\n",
      "0.00010800361633300781\n",
      "====================6000000/8119516====================\n",
      "0.0002570152282714844\n",
      "====================7000000/8119516====================\n",
      "0.00011968612670898438\n",
      "====================8000000/8119516====================\n",
      "0.00023818016052246094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  6, 24,  ...,  0,  0,  0],\n",
       "        [ 6,  8, 10,  ...,  0,  0,  0],\n",
       "        [ 6, 13,  0,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 8, 13, 17,  ...,  0,  0,  0],\n",
       "        [ 4,  7,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_indices_tensor = []\n",
    "number=0\n",
    "for mask in mask_indices:\n",
    "    number+=1\n",
    "    padded = []\n",
    "    try:\n",
    "        a = time.time()\n",
    "        if len(mask)==0:\n",
    "            for x in range(40):\n",
    "                padded.append(0)\n",
    "        else:\n",
    "            padded = padded+mask\n",
    "            for x in range(40-len(padded)):\n",
    "                padded.append(0)\n",
    "        mask_indices_tensor.append(torch.tensor(padded).view(1,40))\n",
    "        if ((number%1000000)==0):\n",
    "            print(\"=\"*20+str(number)+\"/8119516\"+\"=\"*20)\n",
    "            print(time.time()-a)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(type(mask))\n",
    "        print(padded)\n",
    "        time.sleep(100)\n",
    "mask_indices_tensor = torch.cat(tuple(mask_indices_tensor),0)\n",
    "mask_indices_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8119516"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask_indices_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,307,564 training samples\n",
      "811,952 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids_all, attention_masks_all, input_ids_real, mask_indices_tensor)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "torch.save(train_dataloader,path+\"train_dataloader.pickle\")\n",
    "torch.save(validation_dataloader,path+\"validation_dataloader.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "masked_index = 12\n",
    "note_num = 0\n",
    "sent = 1\n",
    "note = data.input_ids_all[note_num]\n",
    "retokenize = [tokenizer.convert_ids_to_tokens(sent) for sent in note]\n",
    "real_sent = [tokenizer.convert_ids_to_tokens(sent) for sent in note]\n",
    "retokenize[sent][masked_index] = '[MASK]'\n",
    "start = retokenize[sent].index('[CLS]')+1\n",
    "end = retokenize[sent].index('[SEP]')\n",
    "retokenize = retokenize[sent][start:end]\n",
    "print(retokenize)\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        retokenize,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "indexed_tokens = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print(indexed_tokens)\n",
    "\n",
    "predictions = model(indexed_tokens, attention_mask = attention_mask)\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0][0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "print('predicted_token', predicted_token)\n",
    "print(\"*\"*80)\n",
    "print(real_sent[sent])\n",
    "print(\"*\"*80)\n",
    "print(\"*\"*80)\n",
    "print(\"*\"*80)\n",
    "\n",
    "for predic in predictions[0][0]:\n",
    "    model.train()\n",
    "    predicted_index = torch.argmax(predic).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "    print('predicted_token', predicted_token)\n",
    "    print(\"*\"*80)\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = 2\n",
    "note_num = 0\n",
    "sent = 1\n",
    "note = data.input_ids_all[note_num]\n",
    "\n",
    "predictions = model(inputa, attention_mask= att)\n",
    "print(att)\n",
    "x=0\n",
    "for predic in predictions[0][0]:\n",
    "    model.train()\n",
    "    predicted_index = torch.argmax(predic).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "    print('predicted_token', predicted_token)\n",
    "    print(real_sent[sent][x])\n",
    "    print(\"*\"*80)\n",
    "    x+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228362"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.load(path+'train_dataloader.pickle')\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2203,  4140,  ...,     0,     0,     0],\n",
      "        [  101, 13866,  2001,  ...,  5475,  2121,   102],\n",
      "        [  101, 19960,   103,  ..., 16029, 19875,   102],\n",
      "        ...,\n",
      "        [  101,   103,  2361,  ...,     0,     0,     0],\n",
      "        [  101,  1038,  2213,  ...,     0,     0,     0],\n",
      "        [  101, 13866,  1012,  ...,     0,     0,     0]])\n",
      "********************\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "********************\n",
      "tensor([[  101,  2203,  4140,  ...,     0,     0,     0],\n",
      "        [  101, 13866,  2001,  ...,  5475,  2121,   102],\n",
      "        [  101, 19960,  2015,  ..., 16029, 19875,   102],\n",
      "        ...,\n",
      "        [  101, 24501,  2361,  ...,     0,     0,     0],\n",
      "        [  101,  1038,  2213,  ...,     0,     0,     0],\n",
      "        [  101, 13866,  1012,  ...,     0,     0,     0]])\n",
      "********************\n",
      "tensor([[15, 17,  0,  ...,  0,  0,  0],\n",
      "        [ 7, 10, 21,  ...,  0,  0,  0],\n",
      "        [ 2,  6,  8,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 1,  3,  0,  ...,  0,  0,  0],\n",
      "        [ 3,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]])\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    for bat in batch:\n",
    "        print(bat)\n",
    "        print(\"*\"*20)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      "hey\n",
      "hey\n",
      "hey\n"
     ]
    }
   ],
   "source": [
    "for stuff in batch[3]:\n",
    "    if list(stuff).count(torch.tensor([0]))==40:\n",
    "        print(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, predictions = model(batch[0], \n",
    "                              attention_mask=batch[1], \n",
    "                              masked_lm_labels=batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.0227, grad_fn=<NllLossBackward>)\n",
      "tensor([[[ -6.3707,  -6.2958,  -6.3173,  ...,  -5.7018,  -5.5271,  -3.8146],\n",
      "         [ -9.4427,  -9.0859,  -9.4761,  ...,  -7.4415,  -7.8268,  -8.0699],\n",
      "         [-12.6911, -12.2078, -12.7045,  ..., -10.1267,  -9.8400, -13.1927],\n",
      "         ...,\n",
      "         [ -5.9527,  -5.5827,  -5.4387,  ...,  -6.8245,  -7.8086,  -9.1423],\n",
      "         [ -3.6681,  -3.2603,  -3.3070,  ...,  -3.7100,  -3.9062,  -3.4631],\n",
      "         [ -2.9827,  -2.7402,  -2.8004,  ...,  -2.3719,  -3.0405,  -4.6882]],\n",
      "\n",
      "        [[ -6.7321,  -6.6498,  -6.6339,  ...,  -5.9721,  -5.8305,  -3.9668],\n",
      "         [ -8.5901,  -9.2019,  -9.0785,  ...,  -9.6726,  -8.3878, -11.1835],\n",
      "         [ -2.9934,  -2.4803,  -2.6671,  ...,  -3.7521,  -2.0293,  -4.4948],\n",
      "         ...,\n",
      "         [ -1.7267,  -1.8782,  -1.3739,  ...,  -0.0876,  -1.1547,  -5.2165],\n",
      "         [ -1.3226,  -1.5561,  -0.9446,  ...,   0.0695,  -0.8232,  -5.2455],\n",
      "         [ -0.1155,   0.1870,   0.3250,  ...,  -0.3670,  -1.1332,  -2.9574]],\n",
      "\n",
      "        [[ -6.7859,  -6.7432,  -6.7441,  ...,  -6.1171,  -5.9373,  -4.0616],\n",
      "         [ -9.5458,  -9.5373,  -9.5173,  ...,  -8.9109,  -9.1796,  -5.3447],\n",
      "         [ -5.0346,  -5.3525,  -5.0744,  ...,  -4.7767,  -5.1929,  -6.0990],\n",
      "         ...,\n",
      "         [ -3.8224,  -3.8629,  -3.7131,  ...,  -4.9409,  -6.2555,  -1.1963],\n",
      "         [ -3.9116,  -3.9398,  -3.7955,  ...,  -4.9756,  -6.2931,  -1.2190],\n",
      "         [ -6.2119,  -6.0377,  -6.0323,  ...,  -6.6949,  -8.3589,  -1.2523]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -6.7876,  -6.7892,  -6.7528,  ...,  -6.1495,  -5.9109,  -3.9722],\n",
      "         [ -8.0977,  -7.8837,  -8.0309,  ...,  -7.7263,  -7.1420,  -4.2371],\n",
      "         [ -2.1854,  -2.3019,  -2.1414,  ...,  -4.2371,  -2.2603,  -1.0631],\n",
      "         ...,\n",
      "         [-15.2176, -14.5382, -14.9519,  ..., -12.1937, -12.8031,  -9.9702],\n",
      "         [-10.6580, -10.7208, -10.7239,  ..., -10.2787,  -9.1758,  -5.9548],\n",
      "         [ -7.4705,  -7.5316,  -7.4501,  ...,  -7.2546,  -7.0514,  -5.6842]],\n",
      "\n",
      "        [[ -6.6715,  -6.6316,  -6.6244,  ...,  -6.0437,  -5.7222,  -3.9614],\n",
      "         [ -7.5932,  -7.6213,  -7.7390,  ...,  -8.1522,  -7.6848,  -6.0259],\n",
      "         [ -8.0680,  -7.9787,  -8.2554,  ...,  -9.0183,  -7.6911,  -5.2867],\n",
      "         ...,\n",
      "         [ -6.5610,  -6.3203,  -6.7827,  ...,  -7.9271,  -6.5453,  -4.5403],\n",
      "         [ -6.7778,  -6.6680,  -7.0585,  ...,  -7.5565,  -6.6360,  -4.7336],\n",
      "         [ -6.5884,  -6.6372,  -6.8194,  ...,  -6.5887,  -6.7314,  -3.5448]],\n",
      "\n",
      "        [[ -6.7722,  -6.7104,  -6.6959,  ...,  -6.1210,  -5.8890,  -4.1346],\n",
      "         [-10.0691,  -9.6000,  -9.8646,  ...,  -8.4536,  -7.6511,  -6.9198],\n",
      "         [ -5.1062,  -5.0731,  -5.4672,  ...,  -5.9387,  -5.6391,  -6.3149],\n",
      "         ...,\n",
      "         [-10.9773, -11.1278, -11.1483,  ..., -11.3096,  -9.6284,  -9.4511],\n",
      "         [ -8.1767,  -7.8361,  -8.0998,  ...,  -7.5314,  -7.9859,  -4.9472],\n",
      "         [-14.1760, -13.9701, -13.9143,  ..., -11.2807, -11.1810,  -7.3328]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def getSent_pred(prediction,real_labels):\n",
    "    sentlist_real = []\n",
    "    sep_list = []\n",
    "    for sent2 in real_labels:\n",
    "        tokenized = tokenizer.convert_ids_to_tokens(sent2)\n",
    "        sep = tokenized.index('[SEP]')\n",
    "        sep_list.append(sep)\n",
    "        sentlist_real.append(tokenized[1:sep])\n",
    "    \n",
    "    \n",
    "    sentlist_ids = []\n",
    "    sentlist = []\n",
    "    for sent in prediction:\n",
    "        word_list = []\n",
    "        for word in sent:\n",
    "            word_list.append(torch.argmax(word))\n",
    "        sentlist_ids.append(word_list)\n",
    "    \n",
    "    for index,sent in enumerate(sentlist_ids):\n",
    "        sentlist.append(tokenizer.convert_ids_to_tokens(sent)[1:sep_list[index]])\n",
    "    return sentlist,sentlist_real\n",
    "p,r = getSent_pred(predictions,batch[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(p[10],r[10],smoothing_function=smoothie)\n",
    "BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13953317517960595"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bleu(p,r):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_list = []\n",
    "    for index in range(len(p)):\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu(p[index],r[index],smoothing_function=smoothie)\n",
    "        bleu_list.append(BLEUscore)\n",
    "    return sum(bleu_list) / len(bleu_list)\n",
    "bleu(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['worse',\n",
       "  '##ning',\n",
       "  'patch',\n",
       "  '##y',\n",
       "  'op',\n",
       "  '##ac',\n",
       "  '##ities',\n",
       "  'and',\n",
       "  'increasing',\n",
       "  'fi',\n",
       "  '##o',\n",
       "  '##2',\n",
       "  'concerning',\n",
       "  'for',\n",
       "  'developing',\n",
       "  'ar',\n",
       "  '##ds',\n",
       "  '.'],\n",
       " ['left', 'atrium', ':', 'mild', 'la', 'en', '##lar', '##gement', '.'],\n",
       " ['fen', '##tan', '##yl', 'ci', '##tra', '##te', '8', '.'],\n",
       " ['fever',\n",
       "  ',',\n",
       "  'unknown',\n",
       "  'origin',\n",
       "  '(',\n",
       "  'fu',\n",
       "  '##o',\n",
       "  ',',\n",
       "  'hyper',\n",
       "  '##ther',\n",
       "  '##mia',\n",
       "  ',',\n",
       "  'p',\n",
       "  '##yre',\n",
       "  '##xia',\n",
       "  ')',\n",
       "  'assessment',\n",
       "  ':',\n",
       "  't',\n",
       "  '##max',\n",
       "  '100',\n",
       "  '.',\n",
       "  '4',\n",
       "  'ha',\n",
       "  '##em',\n",
       "  '##od',\n",
       "  '##ina',\n",
       "  '##mic',\n",
       "  '##ally',\n",
       "  'stable',\n",
       "  'otherwise',\n",
       "  'action',\n",
       "  ':',\n",
       "  'receiving',\n",
       "  'amp',\n",
       "  '##ici',\n",
       "  '##llin',\n",
       "  ',',\n",
       "  'ce',\n",
       "  '##ft',\n",
       "  '##ria',\n",
       "  '##xon',\n",
       "  '##e',\n",
       "  ',',\n",
       "  'az',\n",
       "  '##ith',\n",
       "  '##rom',\n",
       "  '##y',\n",
       "  '##cin',\n",
       "  ',',\n",
       "  'van',\n",
       "  '##com',\n",
       "  '##y',\n",
       "  '##cin',\n",
       "  ',',\n",
       "  'ac',\n",
       "  '##y',\n",
       "  '##cl',\n",
       "  '##ov',\n",
       "  '##ir',\n",
       "  '.'],\n",
       " ['pt',\n",
       "  'receiving',\n",
       "  'lo',\n",
       "  '##press',\n",
       "  '##or',\n",
       "  '10',\n",
       "  'mg',\n",
       "  'iv',\n",
       "  'q',\n",
       "  '6',\n",
       "  'hours',\n",
       "  'which',\n",
       "  'is',\n",
       "  'effective',\n",
       "  'in',\n",
       "  'decreasing',\n",
       "  'hr',\n",
       "  'to',\n",
       "  '80',\n",
       "  '-',\n",
       "  '90',\n",
       "  'ns',\n",
       "  '##r',\n",
       "  'no',\n",
       "  'ec',\n",
       "  '##top',\n",
       "  '##y',\n",
       "  '.'],\n",
       " ['mae', '.'],\n",
       " ['the', 'patient', 'was', 'administered', 'conscious', 'se', '##dation', '.'],\n",
       " ['finally',\n",
       "  ',',\n",
       "  'the',\n",
       "  'cat',\n",
       "  '##het',\n",
       "  '##er',\n",
       "  'was',\n",
       "  'stat',\n",
       "  '##lock',\n",
       "  '##ed',\n",
       "  'into',\n",
       "  'place',\n",
       "  'and',\n",
       "  'a',\n",
       "  'sterile',\n",
       "  'transparent',\n",
       "  'dressing',\n",
       "  'was',\n",
       "  'applied',\n",
       "  '.'],\n",
       " ['ic',\n",
       "  '##u',\n",
       "  'care',\n",
       "  'nutrition',\n",
       "  ':',\n",
       "  't',\n",
       "  '##p',\n",
       "  '##n',\n",
       "  'w',\n",
       "  '/',\n",
       "  'lip',\n",
       "  '##ids',\n",
       "  '-',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  '212',\n",
       "  '##7',\n",
       "  '-',\n",
       "  '10',\n",
       "  '-',\n",
       "  '13',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  '06',\n",
       "  ':',\n",
       "  '40',\n",
       "  'pm',\n",
       "  '41',\n",
       "  '.'],\n",
       " ['ao',\n",
       "  '##rta',\n",
       "  ':',\n",
       "  'mild',\n",
       "  '##y',\n",
       "  'dil',\n",
       "  '##ated',\n",
       "  'ao',\n",
       "  '##rti',\n",
       "  '##c',\n",
       "  'root',\n",
       "  '.'],\n",
       " ['lungs',\n",
       "  ':',\n",
       "  'using',\n",
       "  'accessory',\n",
       "  'muscles',\n",
       "  'to',\n",
       "  'breath',\n",
       "  ',',\n",
       "  'prolonged',\n",
       "  'ex',\n",
       "  '##pi',\n",
       "  '##rator',\n",
       "  '##y',\n",
       "  'phase',\n",
       "  ',',\n",
       "  'ex',\n",
       "  '##pi',\n",
       "  '##rator',\n",
       "  '##y',\n",
       "  'w',\n",
       "  '##hee',\n",
       "  '##zes',\n",
       "  '.'],\n",
       " ['lungs',\n",
       "  ':',\n",
       "  'left',\n",
       "  'lung',\n",
       "  'field',\n",
       "  'demonstrates',\n",
       "  'dull',\n",
       "  '##ness',\n",
       "  'and',\n",
       "  'diminished',\n",
       "  'breath',\n",
       "  'sounds',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'half',\n",
       "  'up',\n",
       "  'the',\n",
       "  'left',\n",
       "  'lung',\n",
       "  'field',\n",
       "  '.'],\n",
       " ['#', '.'],\n",
       " ['i',\n",
       "  'agree',\n",
       "  'with',\n",
       "  'his',\n",
       "  '/',\n",
       "  'her',\n",
       "  'note',\n",
       "  'above',\n",
       "  ',',\n",
       "  'including',\n",
       "  'assessment',\n",
       "  'and',\n",
       "  'plan',\n",
       "  '.'],\n",
       " ['tr',\n",
       "  '##op',\n",
       "  '##oni',\n",
       "  '##n',\n",
       "  'results',\n",
       "  'from',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  '7',\n",
       "  '-',\n",
       "  '25',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  '02',\n",
       "  '##00',\n",
       "  'avail',\n",
       "  'at',\n",
       "  '210',\n",
       "  '##0',\n",
       "  ',',\n",
       "  '+',\n",
       "  '@',\n",
       "  '0',\n",
       "  '.',\n",
       "  '62',\n",
       "  ',',\n",
       "  'this',\n",
       "  'am',\n",
       "  '0',\n",
       "  '.',\n",
       "  '51',\n",
       "  '.'],\n",
       " ['[',\n",
       "  '*',\n",
       "  '*',\n",
       "  'md',\n",
       "  'number',\n",
       "  '(',\n",
       "  '1',\n",
       "  ')',\n",
       "  '207',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  'dictated',\n",
       "  'by',\n",
       "  ':',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  'last',\n",
       "  'name',\n",
       "  '(',\n",
       "  'name',\n",
       "  '##pa',\n",
       "  '##tter',\n",
       "  '##n',\n",
       "  '##1',\n",
       "  ')',\n",
       "  '64',\n",
       "  '##53',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  'med',\n",
       "  '##quist',\n",
       "  '##36',\n",
       "  'd',\n",
       "  ':',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  '219',\n",
       "  '##0',\n",
       "  '-',\n",
       "  '2',\n",
       "  '-',\n",
       "  '3',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  '09',\n",
       "  ':',\n",
       "  '49',\n",
       "  't',\n",
       "  ':',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  '219',\n",
       "  '##0',\n",
       "  '-',\n",
       "  '2',\n",
       "  '-'],\n",
       " ['hc', '##t', 'stable', '.'],\n",
       " ['pt',\n",
       "  'tolerated',\n",
       "  'her',\n",
       "  'clear',\n",
       "  'liquid',\n",
       "  'diet',\n",
       "  'and',\n",
       "  'her',\n",
       "  'diet',\n",
       "  'has',\n",
       "  'been',\n",
       "  'advanced',\n",
       "  'this',\n",
       "  'am',\n",
       "  '.'],\n",
       " ['the', 'ic', '##a', '/', 'cc', '##a', 'ratio', 'is', '1', '.', '1', '.'],\n",
       " ['evaluation',\n",
       "  'of',\n",
       "  'pre',\n",
       "  '##vert',\n",
       "  '##eb',\n",
       "  '##ral',\n",
       "  'soft',\n",
       "  'tissues',\n",
       "  'is',\n",
       "  'limited',\n",
       "  'by',\n",
       "  'end',\n",
       "  '##ot',\n",
       "  '##rac',\n",
       "  '##hea',\n",
       "  '##l',\n",
       "  'and',\n",
       "  'nas',\n",
       "  '##oga',\n",
       "  '##st',\n",
       "  '##ric',\n",
       "  'tubes',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'was',\n",
       "  'able',\n",
       "  'to',\n",
       "  'get',\n",
       "  'him',\n",
       "  'changed',\n",
       "  ',',\n",
       "  'washed',\n",
       "  'up',\n",
       "  ',',\n",
       "  'and',\n",
       "  'brought',\n",
       "  'into',\n",
       "  'the',\n",
       "  'emergency',\n",
       "  'department',\n",
       "  '.'],\n",
       " ['urine',\n",
       "  'toxic',\n",
       "  '##ology',\n",
       "  'screen',\n",
       "  'sent',\n",
       "  'today',\n",
       "  '-',\n",
       "  'positive',\n",
       "  'for',\n",
       "  'cocaine',\n",
       "  'meta',\n",
       "  '##bol',\n",
       "  '##ites',\n",
       "  '.'],\n",
       " ['pun',\n",
       "  '##cta',\n",
       "  '##te',\n",
       "  'h',\n",
       "  '##yp',\n",
       "  '##ode',\n",
       "  '##ns',\n",
       "  '##ity',\n",
       "  'in',\n",
       "  'segment',\n",
       "  'vi',\n",
       "  'measures',\n",
       "  '3',\n",
       "  'mm',\n",
       "  'and',\n",
       "  'is',\n",
       "  'too',\n",
       "  'small',\n",
       "  'to',\n",
       "  'further',\n",
       "  'character',\n",
       "  '##ize',\n",
       "  '.'],\n",
       " ['no', 'evidence', 'of', 'p', '##ne', '##um', '##otho', '##ra', '##x', '.'],\n",
       " ['there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'active',\n",
       "  'extra',\n",
       "  '##vas',\n",
       "  '##ation',\n",
       "  '.'],\n",
       " ['of',\n",
       "  'note',\n",
       "  ',',\n",
       "  'she',\n",
       "  'did',\n",
       "  'not',\n",
       "  'take',\n",
       "  'any',\n",
       "  'of',\n",
       "  'her',\n",
       "  'medications',\n",
       "  'yesterday',\n",
       "  ',',\n",
       "  'including',\n",
       "  'insulin',\n",
       "  'and',\n",
       "  'met',\n",
       "  '##form',\n",
       "  '##in',\n",
       "  '.'],\n",
       " ['use',\n",
       "  'tr',\n",
       "  '##az',\n",
       "  '##odon',\n",
       "  '##e',\n",
       "  '12',\n",
       "  '.',\n",
       "  '5',\n",
       "  'mg',\n",
       "  'hs',\n",
       "  'for',\n",
       "  'sleep',\n",
       "  'as',\n",
       "  'needed',\n",
       "  ',',\n",
       "  'start',\n",
       "  'ty',\n",
       "  '##len',\n",
       "  '##ol',\n",
       "  'standing',\n",
       "  '650',\n",
       "  '##mg',\n",
       "  'q',\n",
       "  '##6',\n",
       "  '##h',\n",
       "  ',',\n",
       "  'please',\n",
       "  'keep',\n",
       "  'in',\n",
       "  'mind',\n",
       "  'non',\n",
       "  '-',\n",
       "  'ph',\n",
       "  '##arm',\n",
       "  '##aco',\n",
       "  '##logic',\n",
       "  'del',\n",
       "  '##iri',\n",
       "  '##um',\n",
       "  'prevention',\n",
       "  ':',\n",
       "  'a',\n",
       "  ')',\n",
       "  'remove',\n",
       "  'all',\n",
       "  'lines',\n",
       "  'and',\n",
       "  'cat',\n",
       "  '##het',\n",
       "  '##ers',\n",
       "  'as',\n",
       "  'soon',\n",
       "  'as',\n",
       "  'possible',\n",
       "  ',',\n",
       "  'es',\n",
       "  '##p',\n",
       "  'foley',\n",
       "  'b',\n",
       "  ')',\n",
       "  'avoid',\n",
       "  'se'],\n",
       " ['the', 'pl', '##eur', '##al', 'spaces', 'are', 'clear', '.'],\n",
       " ['o',\n",
       "  '##ob',\n",
       "  'to',\n",
       "  'chair',\n",
       "  'and',\n",
       "  'com',\n",
       "  '##mo',\n",
       "  '##de',\n",
       "  'with',\n",
       "  'no',\n",
       "  'assistance',\n",
       "  'needed',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'identified',\n",
       "  'having',\n",
       "  'had',\n",
       "  'no',\n",
       "  'children',\n",
       "  ',',\n",
       "  'her',\n",
       "  'failed',\n",
       "  'year',\n",
       "  '##long',\n",
       "  'marriage',\n",
       "  'that',\n",
       "  'ended',\n",
       "  '30',\n",
       "  'years',\n",
       "  'ago',\n",
       "  '(',\n",
       "  'she',\n",
       "  'thinks',\n",
       "  'her',\n",
       "  'husband',\n",
       "  'was',\n",
       "  'probably',\n",
       "  'a',\n",
       "  'homosexual',\n",
       "  'and',\n",
       "  'not',\n",
       "  'well',\n",
       "  'differentiated',\n",
       "  'from',\n",
       "  'his',\n",
       "  'twin',\n",
       "  'brother',\n",
       "  ')',\n",
       "  ',',\n",
       "  'the',\n",
       "  'death',\n",
       "  'of',\n",
       "  'her',\n",
       "  '43',\n",
       "  'y',\n",
       "  '/',\n",
       "  'o',\n",
       "  'father',\n",
       "  'when',\n",
       "  'she',\n",
       "  'was',\n",
       "  '18',\n",
       "  ',',\n",
       "  'and',\n",
       "  'her',\n",
       "  'career',\n",
       "  'and',\n",
       "  'title',\n",
       "  'in',\n",
       "  'ballet',\n",
       "  'as',\n",
       "  'important',\n",
       "  'losses',\n",
       "  '.'],\n",
       " ['mr',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  'known',\n",
       "  'last',\n",
       "  '##name',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  'was',\n",
       "  'treated',\n",
       "  'with',\n",
       "  'a',\n",
       "  'dil',\n",
       "  '##aud',\n",
       "  '##id',\n",
       "  'pc',\n",
       "  '##a',\n",
       "  'for',\n",
       "  'pain',\n",
       "  'control',\n",
       "  '.'],\n",
       " ['will',\n",
       "  'check',\n",
       "  'ck',\n",
       "  'and',\n",
       "  'consult',\n",
       "  'ne',\n",
       "  '##uro',\n",
       "  '/',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  'last',\n",
       "  'name',\n",
       "  '(',\n",
       "  'l',\n",
       "  '##f',\n",
       "  ')',\n",
       "  '93',\n",
       "  '##53',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  ',',\n",
       "  '[',\n",
       "  '*',\n",
       "  '*',\n",
       "  'first',\n",
       "  'name',\n",
       "  '##3',\n",
       "  '(',\n",
       "  'l',\n",
       "  '##f',\n",
       "  ')',\n",
       "  '*',\n",
       "  '*',\n",
       "  ']',\n",
       "  'need',\n",
       "  'to',\n",
       "  'consider',\n",
       "  'w',\n",
       "  '/',\n",
       "  'u',\n",
       "  'for',\n",
       "  'mg',\n",
       "  ',',\n",
       "  'als',\n",
       "  'or',\n",
       "  'other',\n",
       "  'nm',\n",
       "  'disease',\n",
       "  ',',\n",
       "  'including',\n",
       "  'em',\n",
       "  '##g',\n",
       "  '/',\n",
       "  'b',\n",
       "  '##x',\n",
       "  'but',\n",
       "  'will',\n",
       "  'start',\n",
       "  'with',\n",
       "  'consult']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSent_real(real_labels):\n",
    "    \n",
    "getSent_real(batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))\n",
    "print(len(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1010) tensor(3671)\n",
      "tensor(1012) tensor(3726)\n",
      "tensor(3968) tensor(2546)\n",
      "==========\n",
      "tensor(22520) tensor(22520)\n",
      "==========\n",
      "tensor(1997) tensor(2005)\n",
      "==========\n",
      "tensor(1998) tensor(2187)\n",
      "tensor(7913) tensor(7913)\n",
      "tensor(1012) tensor(1011)\n",
      "tensor(1008) tensor(3460)\n",
      "tensor(1012) tensor(1012)\n",
      "==========\n",
      "tensor(1999) tensor(1999)\n",
      "tensor(3531) tensor(3531)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1008) tensor(1031)\n",
      "tensor(28952) tensor(5018)\n",
      "tensor(1008) tensor(1008)\n",
      "==========\n",
      "tensor(19761) tensor(19761)\n",
      "tensor(1010) tensor(1010)\n",
      "tensor(2111) tensor(5776)\n",
      "tensor(1044) tensor(1044)\n",
      "tensor(12184) tensor(12184)\n",
      "tensor(1024) tensor(1010)\n",
      "tensor(6911) tensor(21877)\n",
      "==========\n",
      "tensor(2692) tensor(2487)\n",
      "tensor(1008) tensor(1011)\n",
      "tensor(1008) tensor(2538)\n",
      "tensor(4002) tensor(2260)\n",
      "tensor(2557) tensor(1008)\n",
      "tensor(1007) tensor(1008)\n",
      "tensor(1007) tensor(12528)\n",
      "tensor(1007) tensor(6483)\n",
      "tensor(1007) tensor(1007)\n",
      "tensor(2184) tensor(3114)\n",
      "tensor(2053) tensor(13483)\n",
      "tensor(1997) tensor(2006)\n",
      "tensor(1013) tensor(1011)\n",
      "tensor(6904) tensor(13866)\n",
      "tensor(5776) tensor(11616)\n",
      "tensor(1013) tensor(6187)\n",
      "tensor(1035) tensor(17371)\n",
      "tensor(1035) tensor(2050)\n",
      "==========\n",
      "tensor(15460) tensor(19172)\n",
      "tensor(2001) tensor(2003)\n",
      "==========\n",
      "tensor(3474) tensor(2933)\n",
      "==========\n",
      "tensor(1052) tensor(1016)\n",
      "==========\n",
      "==========\n",
      "tensor(5833) tensor(4853)\n",
      "==========\n",
      "tensor(2038) tensor(2001)\n",
      "tensor(19707) tensor(27753)\n",
      "==========\n",
      "tensor(1008) tensor(3287)\n",
      "tensor(1013) tensor(2007)\n",
      "tensor(1010) tensor(1010)\n",
      "tensor(18178) tensor(19610)\n",
      "tensor(2890) tensor(2890)\n",
      "tensor(1008) tensor(2654)\n",
      "tensor(2063) tensor(10253)\n",
      "==========\n",
      "tensor(1012) tensor(2363)\n",
      "==========\n",
      "==========\n",
      "tensor(1012) tensor(1012)\n",
      "==========\n",
      "tensor(3255) tensor(19340)\n",
      "tensor(3255) tensor(17540)\n",
      "==========\n",
      "tensor(1996) tensor(1037)\n",
      "tensor(1996) tensor(1996)\n",
      "==========\n",
      "tensor(2969) tensor(2128)\n",
      "tensor(2015) tensor(6749)\n",
      "tensor(1012) tensor(1012)\n",
      "==========\n",
      "tensor(17531) tensor(2445)\n",
      "==========\n",
      "tensor(2692) tensor(2683)\n",
      "tensor(1015) tensor(1022)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1031) tensor(12528)\n",
      "tensor(1008) tensor(1031)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1006) tensor(1006)\n",
      "tensor(5890) tensor(22997)\n",
      "tensor(20464) tensor(20464)\n",
      "==========\n",
      "==========\n",
      "tensor(2629) tensor(1058)\n",
      "tensor(2271) tensor(26337)\n",
      "tensor(6573) tensor(2179)\n",
      "tensor(2011) tensor(2005)\n",
      "tensor(7473) tensor(4958)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1008) tensor(2902)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1031) tensor(2487)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1012) tensor(1010)\n",
      "==========\n",
      "tensor(1996) tensor(1996)\n",
      "tensor(2331) tensor(11889)\n",
      "==========\n",
      "tensor(2828) tensor(2828)\n",
      "==========\n",
      "==========\n",
      "tensor(1997) tensor(1997)\n",
      "tensor(1012) tensor(1012)\n",
      "==========\n",
      "tensor(6190) tensor(2527)\n",
      "tensor(2015) tensor(2595)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "tensor(1035) tensor(1035)\n",
      "==========\n",
      "tensor(2001) tensor(2001)\n",
      "tensor(1012) tensor(1012)\n",
      "==========\n",
      "tensor(28228) tensor(28228)\n",
      "tensor(10388) tensor(12646)\n",
      "==========\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1006) tensor(1006)\n",
      "tensor(1033) tensor(1033)\n",
      "tensor(2011) tensor(2011)\n",
      "tensor(1031) tensor(1031)\n",
      "tensor(1007) tensor(1007)\n",
      "tensor(4002) tensor(22234)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(6396) tensor(19960)\n",
      "tensor(1024) tensor(21619)\n",
      "tensor(1024) tensor(1040)\n",
      "tensor(1024) tensor(1024)\n",
      "tensor(2692) tensor(2692)\n",
      "tensor(1008) tensor(1008)\n",
      "tensor(1011) tensor(1011)\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "for sent in batch[3]:\n",
    "    for mask in sent:\n",
    "        if mask!=0:\n",
    "            predicted_index = torch.argmax(predictions[x][int(mask)])\n",
    "            actual = batch[2][x][int(mask)]\n",
    "            print(predicted_index, actual)\n",
    "    print(\"=\"*10)\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3671)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch[2][0][batch[3][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[MASK]']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25374"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
