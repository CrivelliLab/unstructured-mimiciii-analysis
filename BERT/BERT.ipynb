{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "path = \"/global/cscratch1/sd/ajaybati/pickles/\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdict = {}\n",
    "x=0\n",
    "total = 0\n",
    "a=0\n",
    "for note in notes_df2[\"TEXT\"]:\n",
    "    x+=1\n",
    "    start = time.time()\n",
    "    for sent in sent_tokenize(note):\n",
    "        try:\n",
    "            encdict = tokenizer.encode_plus(sent, return_tensors='pt')\n",
    "            length = len(encdict['input_ids'][0])\n",
    "            if length not in maxdict:\n",
    "                maxdict[length] = 1\n",
    "            else:\n",
    "                maxdict[length]+=1\n",
    "        except:\n",
    "            pass\n",
    "    end=time.time()\n",
    "    total+=(end-start)\n",
    "    if x%1000==0:\n",
    "        a+=1\n",
    "        print(x,\"/\",17)\n",
    "        print(total*1/x*(len(notes_df2[\"TEXT\"])-1000*a)/3600)\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "maxdf = pd.DataFrame(data = maxdict).set_index(maxdict.keys())\n",
    "\n",
    "with open(path+'countsentlength.pickle','rb') as f:\n",
    "    maxdf = pickle.load(f)\n",
    "\n",
    "maxdf = pd.DataFrame({'Count Numbers':list(maxdict.keys()),'Frequency':list(maxdict.values())})\n",
    "maxdf[maxdf['Count Numbers']<128]['Frequency'].sum()/maxdf['Frequency'].sum()\n",
    "\n",
    "maxdf['Frequency'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noteevents = pd.read_csv(\"/project/projectdirs/m1532/Projects_MVP/_datasets/mimiciii/NOTEEVENTS.csv\")\n",
    "\n",
    "# salience_patient = pd.read_pickle(path+\"salience_patient.pickle\")\n",
    "\n",
    "\n",
    "\n",
    "final_df = pd.read_pickle(path+\"final_df.pickle\")\n",
    "notes_df = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = final_df[[\"HADM_ID\",\"SUBJECT_ID\",\"DOB\"]].drop_duplicates().merge(notes_df, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"right\")\n",
    "\n",
    "notes = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = pd.merge(final_df[[\"SUBJECT_ID\",\"HADM_ID\",\"DOB\"]],notes.drop_duplicates(subset = [\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\"]), on=[\"SUBJECT_ID\",\"HADM_ID\"], how = \"left\")\n",
    "notes_df = notes_df.drop_duplicates(subset='TEXT')\n",
    "notes_df2 = notes_df[notes_df[\"CATEGORY\"]==\"Discharge summary\"]\n",
    "notes_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data collection\n",
    "import random\n",
    "class textLoader(Dataset):\n",
    "    def __init__(self,transform = None, yes = True):\n",
    "        self.input_ids_all = []\n",
    "        self.attention_masks_all = []\n",
    "        self.text = notes_df2[\"TEXT\"].tolist()\n",
    "        self.samples = len(self.text)\n",
    "        total = 0\n",
    "        x=0\n",
    "        for note in self.text:\n",
    "            start = time.time()\n",
    "            try:\n",
    "                for sent in sent_tokenize(note):\n",
    "                    \n",
    "                    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,\n",
    "                        truncation = True,# Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "                    # Add the encoded sentence to the list.    \n",
    "                    input_ids = encoded_dict['input_ids']\n",
    "                    attention_masks = encoded_dict['attention_mask']\n",
    "\n",
    "                    if yes:\n",
    "                        input_ids_real = []\n",
    "                        for index,word in enumerate(input_ids[0]):\n",
    "                            if int(word) != 101 and int(word) != 102:\n",
    "                                rando = random.random()\n",
    "                                random.seed()\n",
    "                                if rando < 0.2 and int(word)!=0:\n",
    "                                    input_ids_real.append(103)\n",
    "                                else:\n",
    "                                    input_ids_real.append(int(word))\n",
    "                            else:\n",
    "                                input_ids_real.append(int(word))\n",
    "                        input_ids_real = torch.tensor(input_ids_real).view(1,64)\n",
    "\n",
    "                        self.input_ids_all.append(input_ids_real)\n",
    "                        self.attention_masks_all.append(attention_masks)\n",
    "                    else:\n",
    "\n",
    "                        self.input_ids_all.append(input_ids)\n",
    "                        self.attention_masks_all.append(attention_masks)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(str(e), input_ids)\n",
    "            end = time.time()\n",
    "            if((self.text.index(note)+1)%10000==0):\n",
    "                x+=1\n",
    "                total+=(end-start)\n",
    "                print((self.text.index(note)+1)/len(self.text), total/(x))\n",
    "       \n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.input_ids_all[index],self.attention_masks_all[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    \n",
    "\n",
    "data_2 = textLoader(yes = True)\n",
    "print(\"done\")\n",
    "data_input_randomized = torch.cat(tuple(data_2.input_ids_all),dim = 0)\n",
    "attention_masks_randomized = torch.cat(tuple(data_2.attention_masks_all),dim = 0)\n",
    "print(\"save 1\")\n",
    "torch.save(data_input_randomized,\"pickles/input_ids_randomized30.pickle\")\n",
    "print(\"save 2\")\n",
    "torch.save(attention_masks_randomized,\"pickles/attention_masks_randomized30.pickle\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_all = torch.load(path+\"DSmodeldata128.pickle\")\n",
    "attention_masks_all = torch.load(path+\"DSmodeldataAM128.pickle\")\n",
    "input_ids_real = torch.load(path+\"DSmodeldatareal128.pickle\")\n",
    "mask_indices = []\n",
    "x=0\n",
    "for sent in input_ids_all:\n",
    "    start=time.time()\n",
    "    sentence = list(sent)\n",
    "    mask = []\n",
    "    for word in sentence:\n",
    "        if word==103:\n",
    "            mask.append(index)\n",
    "    mask_indices.append(mask)\n",
    "    end = time.time()\n",
    "    x+=1\n",
    "    if x%500000==0:\n",
    "        print(\"=\"*20+\" \"+str(x)+\"/8286324\"+\"=\"*20, len(mask)/len(masked_indices))\n",
    "\n",
    "mask_indices_tensor = []\n",
    "number=0\n",
    "for mask in mask_indices:\n",
    "    number+=1\n",
    "    padded = []\n",
    "    try:\n",
    "        a = time.time()\n",
    "        if len(mask)==0:\n",
    "            for x in range(135):\n",
    "                padded.append(0)\n",
    "        else:\n",
    "            padded = padded+mask\n",
    "            for x in range(135-len(padded)):\n",
    "                padded.append(0)\n",
    "        mask_indices_tensor.append(torch.tensor(padded).view(1,135))\n",
    "        if ((number%500000)==0):\n",
    "            print(\"=\"*20+str(number)+\"/2197122\"+\"=\"*20)\n",
    "            print(time.time()-a)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(type(mask))\n",
    "        print(padded)\n",
    "mask_indices_tensor = torch.cat(tuple(mask_indices_tensor),0)\n",
    "torch.save(mask_indices_tensor,path+'mask_indices_tensorDS128.pickle')\n",
    "\n",
    "mask_indices_tensor = torch.load(path+\"mask_indices_tensorDS128.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,977,409 training samples\n",
      "219,713 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids_all, attention_masks_all, input_ids_real, mask_indices_tensor)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "torch.save(train_dataloader,path+\"train_dataloaderDS128.pickle\")\n",
    "torch.save(validation_dataloader,path+\"validation_dataloaderDS128.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "masked_index = 12\n",
    "note_num = 0\n",
    "sent = 1\n",
    "note = data.input_ids_all[note_num]\n",
    "retokenize = [tokenizer.convert_ids_to_tokens(sent) for sent in note]\n",
    "real_sent = [tokenizer.convert_ids_to_tokens(sent) for sent in note]\n",
    "retokenize[sent][masked_index] = '[MASK]'\n",
    "start = retokenize[sent].index('[CLS]')+1\n",
    "end = retokenize[sent].index('[SEP]')\n",
    "retokenize = retokenize[sent][start:end]\n",
    "print(retokenize)\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        retokenize,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "indexed_tokens = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print(indexed_tokens)\n",
    "\n",
    "predictions = model(indexed_tokens, attention_mask = attention_mask)\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0][0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "\n",
    "for predic in predictions[0][0]:\n",
    "    model.train()\n",
    "    predicted_index = torch.argmax(predic).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "    print('predicted_token', predicted_token)\n",
    "    print(\"*\"*80)\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.load(path+'train_dataloaderDS128.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing using functions\n",
    "len(train_dataloader)\n",
    "\n",
    "train_dataloader = torch.load(path+'train_dataloader.pickle')\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    for bat in batch:\n",
    "        print(bat)\n",
    "        print(\"*\"*20)\n",
    "    break\n",
    "\n",
    "for stuff in batch[3]:\n",
    "    if list(stuff).count(torch.tensor([0]))==40:\n",
    "        print(\"hey\")\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "a = \"The dog loves to play with its chew toy\"\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        a,                      # Sentence to encode.\n",
    "                        add_special_tokens = True,\n",
    "                        truncation=True,# Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 11,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "real = encoded_dict['input_ids']\n",
    "indexed_tokens = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "indexed_tokens= indexed_tokens\n",
    "indexed_tokens[0][8] = 103\n",
    "\n",
    "\n",
    "indexed_tokens\n",
    "\n",
    "loss, predictions = model(indexed_tokens, \n",
    "                              attention_mask=attention_mask, \n",
    "                              masked_lm_labels=real)\n",
    "\n",
    "print(loss)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0, 8]).item() #8 is one of the masked tokens index\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "predicted_token\n",
    "\n",
    "top_k = 10\n",
    "probs = torch.nn.functional.softmax(predictions[0, 8], dim=-1)\n",
    "top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "for i, pred_idx in enumerate(top_k_indices):\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "    token_weight = top_k_weights[i]\n",
    "    print(predicted_token, token_weight)\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def getSent_pred(prediction,real_labels):\n",
    "    sentlist_real = []\n",
    "    sep_list = []\n",
    "    for sent2 in real_labels:\n",
    "        tokenized = tokenizer.convert_ids_to_tokens(sent2)\n",
    "        sep = tokenized.index('[SEP]')\n",
    "        sep_list.append(sep)\n",
    "        sentlist_real.append(tokenized[1:sep])\n",
    "    \n",
    "    \n",
    "    sentlist_ids = []\n",
    "    sentlist = []\n",
    "    for sent in prediction:\n",
    "        word_list = []\n",
    "        for word in sent:\n",
    "            word_list.append(torch.argmax(word))\n",
    "        sentlist_ids.append(word_list)\n",
    "    \n",
    "    for index,sent in enumerate(sentlist_ids):\n",
    "        sentlist.append(tokenizer.convert_ids_to_tokens(sent)[1:sep_list[index]])\n",
    "    return sentlist,sentlist_real\n",
    "p,r = getSent_pred(predictions,batch[2])\n",
    "\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(p[10],r[10],smoothing_function=smoothie)\n",
    "BLEUscore\n",
    "\n",
    "def bleu(p,r):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_list = []\n",
    "    for index in range(len(p)):\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu(p[index],r[index],smoothing_function=smoothie)\n",
    "        bleu_list.append(BLEUscore)\n",
    "    return sum(bleu_list) / len(bleu_list)\n",
    "bleu(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    b_input_ids = batch[0]\n",
    "    b_input_mask = batch[1]\n",
    "    b_input_ids_real = batch[2]\n",
    "    b_input_mask_ids = batch[3]\n",
    "    break\n",
    "    \n",
    "loss, predictions = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        masked_lm_labels=b_input_ids_real)\n",
    "\n",
    "\n",
    "def getSent_pred(prediction,real_labels):\n",
    "    sentlist_real = []\n",
    "    sep_list = []\n",
    "    for sent2 in real_labels:\n",
    "        tokenized = tokenizer.convert_ids_to_tokens(sent2)\n",
    "        sep = tokenized.index('[SEP]')\n",
    "        sep_list.append(sep)\n",
    "        sentlist_real.append(tokenized[1:sep])\n",
    "    \n",
    "    \n",
    "    sentlist_ids = []\n",
    "    sentlist = []\n",
    "    for sent in prediction:\n",
    "        word_list = []\n",
    "        for word in sent:\n",
    "            word_list.append(torch.argmax(word))\n",
    "        sentlist_ids.append(word_list)\n",
    "    \n",
    "    for index,sent in enumerate(sentlist_ids):\n",
    "        sentlist.append(tokenizer.convert_ids_to_tokens(sent)[1:sep_list[index]])\n",
    "    return sentlist,sentlist_real\n",
    "\n",
    "def bleu(p,r):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_list = []\n",
    "    for index in range(len(p)):\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu(p[index],r[index],smoothing_function=smoothie)\n",
    "        bleu_list.append(BLEUscore)\n",
    "    return sum(bleu_list) / len(bleu_list)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def calc_accuracy(prediction, real_labels, mask_indices):\n",
    "    score = 0\n",
    "    total = 0\n",
    "    for step,sent in enumerate(mask_indices):\n",
    "        if list(sent).count(0)!=40:\n",
    "            for mask in sent:\n",
    "                if int(mask)!=0:\n",
    "                    top_k = 10\n",
    "                    probs = torch.nn.functional.softmax(predictions[step, int(mask)], dim=-1)\n",
    "                    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "                    for i, pred_idx in enumerate(top_k_indices):\n",
    "                        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "                        token_weight = top_k_weights[i]\n",
    "                        print(predicted_token, token_weight)\n",
    "                    print('='*80)\n",
    "                    predicted_index = int(torch.argmax(prediction[step,int(mask)]))\n",
    "                    actual = int(real_labels[step][int(mask)])\n",
    "                    if bool(predicted_index==actual):\n",
    "                        score+=1\n",
    "                    total+=1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    p,r = getSent_pred(prediction,real_labels)\n",
    "    \n",
    "    \n",
    "    accuracy = score/total\n",
    "    try:\n",
    "        bscore = bleu(p,r)\n",
    "    except:\n",
    "        bscore = \"Unfortunately, not possible\"\n",
    "    return accuracy, bscore \n",
    "acc, bscore = calc_accuracy(predictions, b_input_ids_real, b_input_mask_ids)\n",
    "print(acc, bscore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
