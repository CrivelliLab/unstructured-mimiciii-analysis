{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/common/software/python/3.7-anaconda-2019.10/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "noteevents = pd.read_csv(\"/project/projectdirs/m1532/Projects_MVP/_datasets/mimiciii/NOTEEVENTS.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>DOB</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>173633</td>\n",
       "      <td>2117-08-07 00:00:00</td>\n",
       "      <td>2141-12-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Admission Date:  [**2141-12-8**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>173633</td>\n",
       "      <td>2117-08-07 00:00:00</td>\n",
       "      <td>2141-12-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Sinus tachycardia.  No significant change comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>173633</td>\n",
       "      <td>2117-08-07 00:00:00</td>\n",
       "      <td>2141-12-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Sinus rhythm with first degree A-V block.  Q-T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>109</td>\n",
       "      <td>173633</td>\n",
       "      <td>2117-08-07 00:00:00</td>\n",
       "      <td>2141-12-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Sinus rhythm.  No significant change compared ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>173633</td>\n",
       "      <td>2117-08-07 00:00:00</td>\n",
       "      <td>2141-12-09</td>\n",
       "      <td>2141-12-09 03:12:00</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Ms [**Known lastname 406**] is a 24 year old w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370137</td>\n",
       "      <td>97492</td>\n",
       "      <td>189314</td>\n",
       "      <td>2126-09-04 00:00:00</td>\n",
       "      <td>2158-08-01</td>\n",
       "      <td>2158-08-01 10:17:00</td>\n",
       "      <td>Rehab Services</td>\n",
       "      <td>TITLE:\\nBEDSIDE SWALLOWING EVALUATION:\\nHISTOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370138</td>\n",
       "      <td>97492</td>\n",
       "      <td>189314</td>\n",
       "      <td>2126-09-04 00:00:00</td>\n",
       "      <td>2158-08-01</td>\n",
       "      <td>2158-08-01 00:34:00</td>\n",
       "      <td>Physician</td>\n",
       "      <td>SICU\\n   HPI:\\n   31 yo RH woman with a PMH de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370139</td>\n",
       "      <td>97492</td>\n",
       "      <td>189314</td>\n",
       "      <td>2126-09-04 00:00:00</td>\n",
       "      <td>2158-08-01</td>\n",
       "      <td>2158-08-01 10:43:00</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>[**2158-8-1**] 10:43 AM\\n MR HEAD W/O CONTRAST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370140</td>\n",
       "      <td>97492</td>\n",
       "      <td>189314</td>\n",
       "      <td>2126-09-04 00:00:00</td>\n",
       "      <td>2158-07-31</td>\n",
       "      <td>2158-07-31 20:52:00</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>[**2158-7-31**] 8:52 PM\\n CTA HEAD W&amp;W/O C &amp; R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370141</td>\n",
       "      <td>97492</td>\n",
       "      <td>189314</td>\n",
       "      <td>2126-09-04 00:00:00</td>\n",
       "      <td>2158-07-31</td>\n",
       "      <td>2158-07-31 20:54:00</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>[**2158-7-31**] 8:54 PM\\n CT HEAD W/O CONTRAST...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367101 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SUBJECT_ID  HADM_ID                  DOB   CHARTDATE  \\\n",
       "0              109   173633  2117-08-07 00:00:00  2141-12-14   \n",
       "1              109   173633  2117-08-07 00:00:00  2141-12-07   \n",
       "2              109   173633  2117-08-07 00:00:00  2141-12-11   \n",
       "3              109   173633  2117-08-07 00:00:00  2141-12-08   \n",
       "4              109   173633  2117-08-07 00:00:00  2141-12-09   \n",
       "...            ...      ...                  ...         ...   \n",
       "370137       97492   189314  2126-09-04 00:00:00  2158-08-01   \n",
       "370138       97492   189314  2126-09-04 00:00:00  2158-08-01   \n",
       "370139       97492   189314  2126-09-04 00:00:00  2158-08-01   \n",
       "370140       97492   189314  2126-09-04 00:00:00  2158-07-31   \n",
       "370141       97492   189314  2126-09-04 00:00:00  2158-07-31   \n",
       "\n",
       "                  CHARTTIME           CATEGORY  \\\n",
       "0                       NaN  Discharge summary   \n",
       "1                       NaN                ECG   \n",
       "2                       NaN                ECG   \n",
       "3                       NaN                ECG   \n",
       "4       2141-12-09 03:12:00            Nursing   \n",
       "...                     ...                ...   \n",
       "370137  2158-08-01 10:17:00     Rehab Services   \n",
       "370138  2158-08-01 00:34:00         Physician    \n",
       "370139  2158-08-01 10:43:00          Radiology   \n",
       "370140  2158-07-31 20:52:00          Radiology   \n",
       "370141  2158-07-31 20:54:00          Radiology   \n",
       "\n",
       "                                                     TEXT  \n",
       "0       Admission Date:  [**2141-12-8**]              ...  \n",
       "1       Sinus tachycardia.  No significant change comp...  \n",
       "2       Sinus rhythm with first degree A-V block.  Q-T...  \n",
       "3       Sinus rhythm.  No significant change compared ...  \n",
       "4       Ms [**Known lastname 406**] is a 24 year old w...  \n",
       "...                                                   ...  \n",
       "370137  TITLE:\\nBEDSIDE SWALLOWING EVALUATION:\\nHISTOR...  \n",
       "370138  SICU\\n   HPI:\\n   31 yo RH woman with a PMH de...  \n",
       "370139  [**2158-8-1**] 10:43 AM\\n MR HEAD W/O CONTRAST...  \n",
       "370140  [**2158-7-31**] 8:52 PM\\n CTA HEAD W&W/O C & R...  \n",
       "370141  [**2158-7-31**] 8:54 PM\\n CT HEAD W/O CONTRAST...  \n",
       "\n",
       "[367101 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.read_pickle(\"pickles/final_df.pickle\")\n",
    "notes_df = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = final_df[[\"HADM_ID\",\"SUBJECT_ID\",\"DOB\"]].drop_duplicates().merge(notes_df, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"right\")\n",
    "\n",
    "notes = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = pd.merge(final_df[[\"SUBJECT_ID\",\"HADM_ID\",\"DOB\"]],notes.drop_duplicates(subset = [\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\"]), on=[\"SUBJECT_ID\",\"HADM_ID\"], how = \"left\")\n",
    "notes_df = notes_df.drop_duplicates(subset='TEXT')\n",
    "notes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "salience_patient = pd.read_pickle(\"pickles/salience_patient.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textLoader(Dataset):\n",
    "    def __init__(self,transform = None):\n",
    "        self.input_ids_all = []\n",
    "        self.attention_masks_all = []\n",
    "        self.text = notes_df[\"TEXT\"].tolist()\n",
    "        self.samples = len(self.text)\n",
    "        total = 0\n",
    "        x=0\n",
    "        for note in self.text:\n",
    "            start = time.time()\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "            try:\n",
    "                for sent in sent_tokenize(note):\n",
    "                    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "                    # Add the encoded sentence to the list.    \n",
    "                    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "                    # And its attention mask (simply differentiates padding from non-padding).\n",
    "                    attention_masks.append(encoded_dict['attention_mask'])\n",
    "            \n",
    "                self.input_ids_all.append(torch.cat(input_ids, dim=0))\n",
    "                self.attention_masks_all.append(torch.cat(attention_masks, dim=0))\n",
    "            except:\n",
    "                pass\n",
    "            end = time.time()\n",
    "            if((self.text.index(note)+1)%1000==0):\n",
    "                x+=1\n",
    "                total+=(end-start)\n",
    "                print((self.text.index(note)+1)/367101, total/(x))\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.input_ids_all[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    \n",
    "data = textLoader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/ajaybati/.local/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "with open('pickles/input_ids.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textLoader(Dataset):\n",
    "    def __init__(self,transform = None):\n",
    "        self.input_ids_all = []\n",
    "        self.attention_masks_all = []\n",
    "        self.text = notes_df[\"TEXT\"].tolist()\n",
    "        self.samples = len(self.text)\n",
    "        total = 0\n",
    "        x=0\n",
    "        for note in self.text:\n",
    "            start = time.time()\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "            try:\n",
    "                for sent in sent_tokenize(note):\n",
    "                    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "                    # Add the encoded sentence to the list.    \n",
    "                    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "                    # And its attention mask (simply differentiates padding from non-padding).\n",
    "                    attention_masks.append(encoded_dict['attention_mask'])\n",
    "            \n",
    "                self.input_ids_all.append(torch.cat(input_ids, dim=0))\n",
    "                self.attention_masks_all.append(torch.cat(attention_masks, dim=0))\n",
    "            except:\n",
    "                pass\n",
    "            end = time.time()\n",
    "            if((self.text.index(note)+1)%1000==0):\n",
    "                x+=1\n",
    "                total+=(end-start)\n",
    "                print((self.text.index(note)+1)/367101, total/(x))\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.input_ids_all[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "    \n",
    "    \n",
    "with open('pickles/input_ids.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 training samples\n",
      "    2 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(data.input_ids_all[0], data.attention_masks_all[0])\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_token ['day'] tensor(1.9854, grad_fn=<NllLossBackward>)\n",
      "['[CLS]', 'demographics', '[MASK]', 'of', 'int', '##uba', '##tion', ':', 'day', 'of', 'mechanical', 'ventilation', ':', '14', 'ideal', 'body', 'weight', ':', '75', '.', '3', 'none', 'ideal', 'tidal', 'volume', ':', '301', '.', '2', '/', '451', '.', '8', '/', '60', '##2', '.', '4', 'ml', '/', 'kg', 'air', '##way', 'air', '##way', 'placement', 'data', 'known', 'difficult', 'int', '##uba', '##tion', ':', 'yes', 'tube', 'type', 'tr', '##ache', '##ost', '##omy', 'tube', ':', 'type', '[SEP]']\n",
      "********************************************************************************\n",
      "['[CLS]', 'demographics', 'day', 'of', 'int', '##uba', '##tion', ':', 'day', 'of', 'mechanical', 'ventilation', ':', '14', 'ideal', 'body', 'weight', ':', '75', '.', '3', 'none', 'ideal', 'tidal', 'volume', ':', '301', '.', '2', '/', '451', '.', '8', '/', '60', '##2', '.', '4', 'ml', '/', 'kg', 'air', '##way', 'air', '##way', 'placement', 'data', 'known', 'difficult', 'int', '##uba', '##tion', ':', 'yes', 'tube', 'type', 'tr', '##ache', '##ost', '##omy', 'tube', ':', 'type', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "masked_index = 2\n",
    "sentence_inNOTE1 = 360000\n",
    "word = 0\n",
    "testing = data.input_ids_all[sentence_inNOTE1]\n",
    "retokenize = [tokenizer.convert_ids_to_tokens(sent) for sent in testing]\n",
    "real_sent = [tokenizer.convert_ids_to_tokens(sent) for sent in testing]\n",
    "retokenize[word][masked_index] = '[MASK]'\n",
    "retokenize = retokenize[word]\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(retokenize)\n",
    "indexed_tokens = tokenizer.build_inputs_with_special_tokens(indexed_tokens)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "loss, predictions = model(tokens_tensor, masked_lm_labels = tokens_tensor)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index+1]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "print('predicted_token', predicted_token , loss)\n",
    "print(retokenize2)\n",
    "print(\"*\"*80)\n",
    "print(real_sent[word])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
