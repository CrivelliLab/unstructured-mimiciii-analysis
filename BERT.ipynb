{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "path = \"/global/cscratch1/sd/ajaybati/pickles/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/common/software/python/3.7-anaconda-2019.10/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "noteevents = pd.read_csv(\"/project/projectdirs/m1532/Projects_MVP/_datasets/mimiciii/NOTEEVENTS.csv\")\n",
    "\n",
    "\n",
    "final_df = pd.read_pickle(path+\"final_df.pickle\")\n",
    "notes_df = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = final_df[[\"HADM_ID\",\"SUBJECT_ID\",\"DOB\"]].drop_duplicates().merge(notes_df, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"right\")\n",
    "\n",
    "notes = noteevents[[\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"CATEGORY\",\"TEXT\"]]\n",
    "notes_df = pd.merge(final_df[[\"SUBJECT_ID\",\"HADM_ID\",\"DOB\"]],notes.drop_duplicates(subset = [\"SUBJECT_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\"]), on=[\"SUBJECT_ID\",\"HADM_ID\"], how = \"left\")\n",
    "notes_df = notes_df.drop_duplicates(subset='TEXT')\n",
    "notes_df\n",
    "\n",
    "salience_patient = pd.read_pickle(path+\"salience_patient.pickle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data collection\n",
    "import random\n",
    "class textLoader(Dataset):\n",
    "    def __init__(self,transform = None, yes = True):\n",
    "        self.input_ids_all = []\n",
    "        self.attention_masks_all = []\n",
    "        self.text = notes_df[\"TEXT\"].tolist()\n",
    "        self.samples = len(self.text)\n",
    "        total = 0\n",
    "        x=0\n",
    "        for note in self.text[250000:]:\n",
    "            start = time.time()\n",
    "            try:\n",
    "                for sent in sent_tokenize(note):\n",
    "                    \n",
    "                    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,\n",
    "                        truncation = True,# Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "                    # Add the encoded sentence to the list.    \n",
    "                    input_ids = encoded_dict['input_ids']\n",
    "                    attention_masks = encoded_dict['attention_mask']\n",
    "\n",
    "                    if yes:\n",
    "                        input_ids_real = []\n",
    "                        for index,word in enumerate(input_ids[0]):\n",
    "                            if int(word) != 101 and int(word) != 102:\n",
    "                                rando = random.random()\n",
    "                                random.seed()\n",
    "                                if rando < 0.2 and int(word)!=0:\n",
    "                                    input_ids_real.append(103)\n",
    "                                else:\n",
    "                                    input_ids_real.append(int(word))\n",
    "                            else:\n",
    "                                input_ids_real.append(int(word))\n",
    "                        input_ids_real = torch.tensor(input_ids_real).view(1,64)\n",
    "\n",
    "                        self.input_ids_all.append(input_ids_real)\n",
    "                        self.attention_masks_all.append(attention_masks)\n",
    "                    else:\n",
    "\n",
    "                        self.input_ids_all.append(input_ids)\n",
    "                        self.attention_masks_all.append(attention_masks)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(str(e), input_ids)\n",
    "            end = time.time()\n",
    "            if((self.text.index(note)+1)%10000==0):\n",
    "                x+=1\n",
    "                total+=(end-start)\n",
    "                print((self.text.index(note)+1)/367101+0.6810114927499517, total/(x))\n",
    "       \n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        return self.input_ids_all[index],self.attention_masks_all[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    \n",
    "print(\"=\"*20,\"data randomized 0.6810114927499517+\",\"=\"*20)\n",
    "\n",
    "data_2 = textLoader(yes = True)\n",
    "print(\"done\")\n",
    "data_input_randomized = torch.cat(tuple(data_2.input_ids_all),dim = 0)\n",
    "attention_masks_randomized = torch.cat(tuple(data_2.attention_masks_all),dim = 0)\n",
    "print(\"save 1\")\n",
    "torch.save(data_input_randomized,\"pickles/input_ids_randomized30.pickle\")\n",
    "print(\"save 2\")\n",
    "torch.save(attention_masks_randomized,\"pickles/attention_masks_randomized30.pickle\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8119516\n"
     ]
    }
   ],
   "source": [
    "input_ids_randomized70 = torch.load(path+\"input_ids_randomized70.pickle\")\n",
    "attention_masks_randomized70 = torch.load(path+\"attention_masks_randomized70.pickle\")\n",
    "REDOinput_ids_randomized30 = torch.load(path+\"REDOinput_ids_randomized30.pickle\")\n",
    "REDOattention_masks_randomized30 = torch.load(path+\"REDOattention_masks_randomized30.pickle\")\n",
    "print(len(input_ids_randomized70)+len(REDOinput_ids_randomized30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8119516"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing data before save\n",
    "fullist2 = list(attention_masks_randomized70)\n",
    "for x in REDOattention_masks_randomized30:\n",
    "    fullist2.append(x)\n",
    "\n",
    "\n",
    "fullist = list(input_ids_randomized70)\n",
    "for x in REDOinput_ids_randomized30:\n",
    "    fullist.append(x)\n",
    "\n",
    "\n",
    "fullist_changed2 = []\n",
    "for tens in fullist2:\n",
    "    tens = tens.view(1,64)\n",
    "    fullist_changed2.append(tens)\n",
    "fullist_changed2\n",
    "\n",
    "fullist_changed = []\n",
    "for tens in fullist:\n",
    "    tens = tens.view(1,64)\n",
    "    fullist_changed.append(tens)\n",
    "fullist_changed\n",
    "\n",
    "attention_masks_randomized100 = torch.cat(tuple(fullist_changed2),0)\n",
    "len(attention_masks_randomized100)\n",
    "\n",
    "input_ids_randomized100 = torch.cat(tuple(fullist_changed),0)\n",
    "len(input_ids_randomized100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_all = torch.load(path+\"input_ids_randomized100.pickle\")\n",
    "attention_masks_all = torch.load(path+\"attention_masks_randomized100.pickle\")\n",
    "input_ids_real = torch.load(path+\"input_ids_real.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 21442, 26915,  6633,  2318,   103,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "tensor([  101, 21442, 26915,  6633,  2318,  2651,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_all[5501010])\n",
    "print(input_ids_real[5501010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_indices = []\n",
    "# x=0\n",
    "# for sent in input_ids_all:\n",
    "#     start=time.time()\n",
    "#     sentence = list(sent)\n",
    "#     mask = []\n",
    "#     for word in sentence:\n",
    "#         if word==103:\n",
    "#             mask.append(index)\n",
    "#     mask_indices.append(mask)\n",
    "#     end = time.time()\n",
    "#     x+=1\n",
    "#     if x%500000==0:\n",
    "#         print(\"=\"*20+\" \"+str(x)+\"/8286324\"+\"=\"*20, len(mask)/len(masked_indices))\n",
    "with open(path+\"mask_indices.pickle\",\"rb\") as f:\n",
    "    mask_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================1000000/8119516====================\n",
      "0.00010085105895996094\n",
      "====================2000000/8119516====================\n",
      "0.00011396408081054688\n",
      "====================3000000/8119516====================\n",
      "0.00010228157043457031\n",
      "====================4000000/8119516====================\n",
      "0.0001289844512939453\n",
      "====================5000000/8119516====================\n",
      "0.00010800361633300781\n",
      "====================6000000/8119516====================\n",
      "0.0002570152282714844\n",
      "====================7000000/8119516====================\n",
      "0.00011968612670898438\n",
      "====================8000000/8119516====================\n",
      "0.00023818016052246094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  6, 24,  ...,  0,  0,  0],\n",
       "        [ 6,  8, 10,  ...,  0,  0,  0],\n",
       "        [ 6, 13,  0,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 8, 13, 17,  ...,  0,  0,  0],\n",
       "        [ 4,  7,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_indices_tensor = []\n",
    "number=0\n",
    "for mask in mask_indices:\n",
    "    number+=1\n",
    "    padded = []\n",
    "    try:\n",
    "        a = time.time()\n",
    "        if len(mask)==0:\n",
    "            for x in range(40):\n",
    "                padded.append(0)\n",
    "        else:\n",
    "            padded = padded+mask\n",
    "            for x in range(40-len(padded)):\n",
    "                padded.append(0)\n",
    "        mask_indices_tensor.append(torch.tensor(padded).view(1,40))\n",
    "        if ((number%1000000)==0):\n",
    "            print(\"=\"*20+str(number)+\"/8119516\"+\"=\"*20)\n",
    "            print(time.time()-a)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(type(mask))\n",
    "        print(padded)\n",
    "        time.sleep(100)\n",
    "mask_indices_tensor = torch.cat(tuple(mask_indices_tensor),0)\n",
    "mask_indices_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8119516"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask_indices_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,307,564 training samples\n",
      "811,952 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids_all, attention_masks_all, input_ids_real, mask_indices_tensor)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "torch.save(train_dataloader,path+\"train_dataloader.pickle\")\n",
    "torch.save(validation_dataloader,path+\"validation_dataloader.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "masked_index = 12\n",
    "note_num = 0\n",
    "sent = 1\n",
    "note = data.input_ids_all[note_num]\n",
    "retokenize = [tokenizer.convert_ids_to_tokens(sent) for sent in note]\n",
    "real_sent = [tokenizer.convert_ids_to_tokens(sent) for sent in note]\n",
    "retokenize[sent][masked_index] = '[MASK]'\n",
    "start = retokenize[sent].index('[CLS]')+1\n",
    "end = retokenize[sent].index('[SEP]')\n",
    "retokenize = retokenize[sent][start:end]\n",
    "print(retokenize)\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        retokenize,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "indexed_tokens = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print(indexed_tokens)\n",
    "\n",
    "predictions = model(indexed_tokens, attention_mask = attention_mask)\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0][0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "print('predicted_token', predicted_token)\n",
    "print(\"*\"*80)\n",
    "print(real_sent[sent])\n",
    "print(\"*\"*80)\n",
    "print(\"*\"*80)\n",
    "print(\"*\"*80)\n",
    "\n",
    "for predic in predictions[0][0]:\n",
    "    model.train()\n",
    "    predicted_index = torch.argmax(predic).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "    print('predicted_token', predicted_token)\n",
    "    print(\"*\"*80)\n",
    "    x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = 2\n",
    "note_num = 0\n",
    "sent = 1\n",
    "note = data.input_ids_all[note_num]\n",
    "\n",
    "predictions = model(inputa, attention_mask= att)\n",
    "print(att)\n",
    "x=0\n",
    "for predic in predictions[0][0]:\n",
    "    model.train()\n",
    "    predicted_index = torch.argmax(predic).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "    print('predicted_token', predicted_token)\n",
    "    print(real_sent[sent][x])\n",
    "    print(\"*\"*80)\n",
    "    x+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
